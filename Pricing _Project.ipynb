{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Portfolio Construction and Risk Analysis\n",
    "\n",
    "This Jupyter notebook demonstrates the construction of various investment portfolios using different strategies and performs risk analysis using Monte Carlo simulations. The project showcases skills in quantitative finance, coding, and time series analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data Collection](#data-collection)\n",
    "3. [Portfolio Construction](#portfolio-construction)\n",
    "    - [Normal Portfolio](#normal-portfolio)\n",
    "    - [Growth Portfolio](#growth-portfolio)\n",
    "    - [Value Portfolio](#value-portfolio)\n",
    "    - [Diversified Portfolio](#diversified-portfolio)\n",
    "4. [Risk Analysis](#risk-analysis)\n",
    "    - [Monte Carlo Simulation](#monte-carlo-simulation)\n",
    "    - [Risk Metrics Calculation](#risk-metrics-calculation)\n",
    "5. [Fund of Funds (FoF) Construction](#fund-of-funds-fof-construction)\n",
    "6. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we construct investment portfolios based on different strategies using data from the S&P 500 companies. We then perform risk analysis on these portfolios to understand their risk-return profiles. The main objectives are:\n",
    "\n",
    "- **Portfolio Construction**: Build portfolios using various strategies (Normal, Growth, Value, Diversification) and different Assets Under Management (AUM) sizes (Small, Medium, Large).\n",
    "- **Risk Analysis**: Calculate risk metrics such as Value at Risk (VaR), Expected Shortfall (ES), and volatility.\n",
    "- **Optimization**: Use Monte Carlo simulations to optimize portfolio weights.\n",
    "- **Fund of Funds (FoF)**: Combine individual portfolios into a FoF to achieve diversification benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "We collect historical price and volume data for S&P 500 companies using the `yfinance` library. We also obtain sector information from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import ssl\n",
    "import certifi\n",
    "import urllib.request\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the annual risk-free rate (e.g., 0.5%)\n",
    "risk_free_rate = 0.005  # 0.5% annualized risk-free rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching S&P 500 Tickers and Sector Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SSL context to use certifi's certificate bundle\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Set environment variable for requests (used by yfinance)\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "# Fetch S&P 500 tickers from Wikipedia using the custom SSL context\n",
    "sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "response = urllib.request.urlopen(sp500_url, context=ssl_context)\n",
    "html = response.read()\n",
    "\n",
    "# Read the HTML tables from the content\n",
    "sp500_table = pd.read_html(html)\n",
    "sp500_df = sp500_table[0]\n",
    "\n",
    "# Fix tickers with dots (e.g., BRK.B -> BRK-B)\n",
    "sp500_df['Symbol'] = sp500_df['Symbol'].str.replace('.', '-', regex=False)\n",
    "\n",
    "# Remove any duplicates or NaN values in the tickers\n",
    "sp500_df = sp500_df.dropna(subset=['Symbol']).drop_duplicates(subset=['Symbol'])\n",
    "\n",
    "# Convert tickers to a list\n",
    "tickers = sp500_df['Symbol'].tolist()\n",
    "\n",
    "# Remove problematic tickers manually (if known)\n",
    "problematic_tickers = ['BF-B', 'BRK-B']\n",
    "tickers = [ticker for ticker in tickers if ticker not in problematic_tickers]\n",
    "\n",
    "# Print the number of tickers\n",
    "print(f\"Number of tickers to download: {len(tickers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for all tickers at once\n",
    "print(\"Downloading data for all tickers...\")\n",
    "try:\n",
    "    # Fetch 2 years of daily price data for each ticker\n",
    "    data = yf.download(tickers, period='2y', interval='1d', group_by='ticker', threads=True)\n",
    "except Exception as e:\n",
    "    # Handle any errors during data download\n",
    "    print(f\"An error occurred while downloading data: {e}\")\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "# Check if the data DataFrame is empty\n",
    "if data.empty:\n",
    "    print(\"No data downloaded.\")\n",
    "else:\n",
    "    # Prepare the data based on the type of column index\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Adjust for MultiIndex by selecting 'Adj Close' prices for each ticker\n",
    "        data = data.stack(level=0)['Adj Close'].unstack()\n",
    "    else:\n",
    "        # If single-level columns, directly use 'Adj Close' prices\n",
    "        data = data['Adj Close']\n",
    "\n",
    "    # Drop tickers with all NaN values\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "\n",
    "    # Filter out tickers with less than 126 data points (insufficient data)\n",
    "    sufficient_data_tickers = [ticker for ticker in data.columns if data[ticker].dropna().shape[0] >= 126]\n",
    "    data = data[sufficient_data_tickers]\n",
    "\n",
    "    # Calculate daily returns by percentage change and drop NaN values\n",
    "    daily_returns = data.pct_change().dropna()\n",
    "\n",
    "    # Print dimensions of the cleaned data DataFrame\n",
    "    print(f\"The data DataFrame has {len(data)} rows and {len(data.columns)} columns (tickers).\")\n",
    "\n",
    "    # Calculate approximate 6-month total returns if sufficient data is available\n",
    "    if len(data) >= 126:\n",
    "        returns_6m = data.iloc[-1] / data.iloc[-126] - 1  # 126 trading days = 6 months approx.\n",
    "        # Proceed with the rest of the calculations\n",
    "    else:\n",
    "        print(\"Not enough data to calculate 6-month returns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Returns, Price, Volume and Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating Returns and Volatility\n",
    "# Calculate 6-month total returns if there is sufficient data\n",
    "if len(data) >= 126:\n",
    "    returns_6m = data.iloc[-1] / data.iloc[-126] - 1  # Approximate 126 trading days in 6 months\n",
    "else:\n",
    "    print(\"Not enough data to calculate 6-month returns.\")\n",
    "\n",
    "# Download and isolate volume data for the past year\n",
    "volume_data = yf.download(tickers, period='1y', interval='1d')['Volume']\n",
    "\n",
    "# Calculate average trading volume over the last 6 months (126 trading days)\n",
    "avg_volume_6m = volume_data.iloc[-126:].mean()\n",
    "\n",
    "# Calculate 6-month volatilities based on the daily returns\n",
    "volatility_6m = daily_returns.iloc[-126:].std()\n",
    "\n",
    "# Retrieve the latest adjusted closing prices for each ticker\n",
    "last_prices = data.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Stock DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Stock DataFrame\n",
    "\n",
    "# Get the union of all tickers from the three Series to ensure alignment\n",
    "all_tickers = returns_6m.index.union(volatility_6m.index).union(avg_volume_6m.index)\n",
    "\n",
    "# Reindex each Series to the common ticker index, filling missing values with NaN\n",
    "returns_6m = returns_6m.reindex(all_tickers)\n",
    "volatility_6m = volatility_6m.reindex(all_tickers)\n",
    "avg_volume_6m = avg_volume_6m.reindex(all_tickers)\n",
    "\n",
    "# Create a DataFrame containing stock data with key metrics\n",
    "stock_data = pd.DataFrame({\n",
    "    'Last_Prices': last_prices,\n",
    "    '6M_Return': returns_6m,\n",
    "    '6M_Volatility': volatility_6m,\n",
    "    'Avg_Volume_6M': avg_volume_6m\n",
    "})\n",
    "\n",
    "# Reset index to make tickers a column in stock_data DataFrame\n",
    "stock_data.reset_index(inplace=True)\n",
    "stock_data.rename(columns={'index': 'Ticker'}, inplace=True)\n",
    "\n",
    "# Merge stock_data with sector information from the S&P 500 DataFrame\n",
    "stock_data = stock_data.merge(\n",
    "    sp500_df[['Symbol', 'GICS Sector']],\n",
    "    left_on='Ticker',\n",
    "    right_on='Symbol',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the redundant 'Symbol' column after merging\n",
    "stock_data.drop('Symbol', axis=1, inplace=True)\n",
    "\n",
    "# Rename 'GICS Sector' column to 'Sector' for clarity\n",
    "stock_data.rename(columns={'GICS Sector': 'Sector'}, inplace=True)\n",
    "\n",
    "# Display the final stock_data DataFrame\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Construction\n",
    "\n",
    "We construct four types of portfolios based on different investment strategies, 20 stocks each:\n",
    "\n",
    "1. **Normal Portfolio**: Top stocks by average trading volume.\n",
    "2. **Growth Portfolio**: Top stocks by 6-month returns.\n",
    "3. **Value Portfolio**: Top stocks with lowest 6-month volatility.\n",
    "4. **Diversified Portfolio**: One stock from each sector.\n",
    "\n",
    "We also define three AUM sizes:\n",
    "\n",
    "- Small Portfolio: \\$50 million\n",
    "- Medium Portfolio: \\$200 million\n",
    "- Large Portfolio: \\$1.5 billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Assets Under Management (AUM) values\n",
    "aum_values = {\n",
    "    'Small Portfolio': 50_000_000,\n",
    "    'Medium Portfolio': 200_000_000,\n",
    "    'Large Portfolio': 1_500_000_000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Portfolio\n",
    "\n",
    "Select the top 20 stocks by average trading volume over the past 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 20 stocks by average volume over the last 6 months\n",
    "top_normal_stocks = stock_data.sort_values(by='Avg_Volume_6M', ascending=False).head(20)\n",
    "\n",
    "# Create the normal portfolio DataFrame\n",
    "normal_portfolio = top_normal_stocks.copy()\n",
    "\n",
    "# Assign initial equal weights\n",
    "normal_portfolio['Initial_Weight'] = 1 / len(normal_portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth Portfolio\n",
    "\n",
    "Select the top 20 stocks based on 6-month total returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 20 stocks sorted by highest 6-month returns\n",
    "top_growth_stocks = stock_data.sort_values(by='6M_Return', ascending=False).head(20).copy()\n",
    "\n",
    "# Calculate initial weights based on proportion of each stock's return to total return\n",
    "top_growth_stocks['Initial_Weight'] = top_growth_stocks['6M_Return'] / top_growth_stocks['6M_Return'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Portfolio\n",
    "\n",
    "Select the top 20 stocks with the lowest 6-month volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 20 stocks sorted by lowest 6-month volatility\n",
    "top_value_stocks = stock_data.sort_values(by='6M_Volatility').head(20).copy()\n",
    "\n",
    "# Calculate the inverse of volatility for weighting purposes\n",
    "top_value_stocks['Inv_Volatility'] = 1 / top_value_stocks['6M_Volatility']\n",
    "\n",
    "# Calculate initial weights based on inverse volatility proportions\n",
    "top_value_stocks['Initial_Weight'] = top_value_stocks['Inv_Volatility'] / top_value_stocks['Inv_Volatility'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversified Portfolio\n",
    "\n",
    "Select one stock from each sector, prioritizing those with the highest 6-month returns within their sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get unique sectors present in the stock_data DataFrame\n",
    "sectors = stock_data['Sector'].dropna().unique()\n",
    "\n",
    "# Select one top-return stock per sector\n",
    "selected_stocks = []\n",
    "for sector in sectors:\n",
    "    # Filter stocks within the current sector\n",
    "    sector_stocks = stock_data[stock_data['Sector'] == sector]\n",
    "    if not sector_stocks.empty:\n",
    "        # Select the stock with the highest 6-month return in the sector\n",
    "        stock = sector_stocks.sort_values(by='6M_Return', ascending=False).head(1)\n",
    "        selected_stocks.append(stock)\n",
    "\n",
    "# Concatenate selected stocks into a diversified portfolio\n",
    "diversified_portfolio = pd.concat(selected_stocks)\n",
    "\n",
    "# Add more high-return stocks until the portfolio has 20 stocks\n",
    "while len(diversified_portfolio) < 20:\n",
    "    remaining_stocks = stock_data[~stock_data['Ticker'].isin(diversified_portfolio['Ticker'])]\n",
    "    # Select the next highest return stock not already in the portfolio\n",
    "    next_stock = remaining_stocks.sort_values(by='6M_Return', ascending=False).head(1)\n",
    "    diversified_portfolio = pd.concat([diversified_portfolio, next_stock])\n",
    "\n",
    "# Set equal initial weights for all stocks in the diversified portfolio\n",
    "diversified_portfolio['Initial_Weight'] = 1 / len(diversified_portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Process Portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_portfolio(portfolio_df, last_prices, aum, strategy_name):\n",
    "    \"\"\"\n",
    "    Process a portfolio by adjusting weights, capping, and calculating quantities based on AUM.\n",
    "\n",
    "    Parameters:\n",
    "    - portfolio_df: DataFrame containing portfolio tickers and initial weights.\n",
    "    - last_prices: Series mapping tickers to their latest prices.\n",
    "    - aum: Available capital (Assets Under Management) for allocation.\n",
    "    - strategy_name: Name of the strategy for labeling purposes.\n",
    "\n",
    "    Returns:\n",
    "    - portfolio_df: Processed DataFrame with final weights, quantities, and financial allocations.\n",
    "    \"\"\"\n",
    "    portfolio_df = portfolio_df.copy()\n",
    "    \n",
    "    # Map the latest price for each ticker\n",
    "    portfolio_df['Price'] = portfolio_df['Ticker'].map(last_prices)\n",
    "    \n",
    "    # Remove rows with missing or zero prices\n",
    "    portfolio_df = portfolio_df.dropna(subset=['Price'])\n",
    "    portfolio_df = portfolio_df[portfolio_df['Price'] > 0]\n",
    "    \n",
    "    # Cap weights at a maximum of 10%\n",
    "    portfolio_df['Capped_Weight'] = portfolio_df['Initial_Weight'].apply(lambda x: min(x, 0.10))\n",
    "    \n",
    "    # Calculate total of capped weights for redistribution check\n",
    "    total_capped_weight = portfolio_df['Capped_Weight'].sum()\n",
    "    if total_capped_weight == 0:\n",
    "        raise ValueError(f\"Total capped weight is zero for {strategy_name} strategy. Cannot proceed with weight redistribution.\")\n",
    "    elif total_capped_weight < 1.0:\n",
    "        # Normalize if total capped weight is less than 1\n",
    "        portfolio_df['Final_Weight'] = portfolio_df['Capped_Weight'] / total_capped_weight\n",
    "    else:\n",
    "        # Redistribute excess weight proportionally for capped weights below 10%\n",
    "        excess_weight = portfolio_df['Initial_Weight'].sum() - portfolio_df['Capped_Weight'].sum()\n",
    "        remaining_weight = portfolio_df['Capped_Weight'].sum()\n",
    "        if remaining_weight == 0:\n",
    "            raise ValueError(f\"Remaining weight is zero for {strategy_name} strategy. Cannot redistribute excess weight.\")\n",
    "        portfolio_df['Final_Weight'] = portfolio_df.apply(\n",
    "            lambda row: row['Capped_Weight'] + (excess_weight * row['Capped_Weight'] / remaining_weight)\n",
    "            if row['Capped_Weight'] < 0.10 else row['Capped_Weight'], axis=1)\n",
    "        # Normalize final weights\n",
    "        portfolio_df['Final_Weight'] /= portfolio_df['Final_Weight'].sum()\n",
    "    \n",
    "    # Remove rows with NaN or zero in 'Final_Weight'\n",
    "    portfolio_df = portfolio_df.dropna(subset=['Final_Weight'])\n",
    "    portfolio_df = portfolio_df[portfolio_df['Final_Weight'] > 0]\n",
    "    \n",
    "    # Calculate 'Financial' allocation based on final weights and AUM\n",
    "    portfolio_df['Financial'] = portfolio_df['Final_Weight'] * aum\n",
    "    \n",
    "    # Drop rows with NaN or zero in 'Financial' or 'Price' before calculating 'Quantity'\n",
    "    portfolio_df = portfolio_df.dropna(subset=['Financial', 'Price'])\n",
    "    portfolio_df = portfolio_df[(portfolio_df['Financial'] > 0) & (portfolio_df['Price'] > 0)]\n",
    "    \n",
    "    # Calculate 'Quantity' as an integer count of shares for each stock\n",
    "    portfolio_df['Quantity'] = (portfolio_df['Financial'] // portfolio_df['Price']).astype(int)\n",
    "    \n",
    "    # Exclude rows where 'Quantity' is zero or negative\n",
    "    portfolio_df = portfolio_df[portfolio_df['Quantity'] > 0]\n",
    "    \n",
    "    # Recalculate 'Financial' based on actual 'Quantity' and 'Price'\n",
    "    portfolio_df['Financial'] = portfolio_df['Quantity'] * portfolio_df['Price']\n",
    "    \n",
    "    # Label portfolio with the strategy name\n",
    "    portfolio_df['Strategy'] = strategy_name\n",
    "    \n",
    "    # Reorder columns for final output\n",
    "    portfolio_df = portfolio_df[['Strategy', 'Ticker', 'Quantity', 'Price', 'Financial']]\n",
    "    \n",
    "    return portfolio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Portfolios for Each AUM Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store processed portfolios\n",
    "portfolios = {}\n",
    "\n",
    "# Loop through each AUM category to create portfolios\n",
    "for aum_name, aum_value in aum_values.items():\n",
    "    try:\n",
    "        # Process Normal Portfolio for the current AUM\n",
    "        portfolio_name = f\"Normal_{aum_name.replace(' ', '_')}\"\n",
    "        processed_portfolio = process_portfolio(normal_portfolio, last_prices, aum_value, 'Normal')\n",
    "        portfolios[portfolio_name] = processed_portfolio\n",
    "        \n",
    "        # Process Growth Portfolio for the current AUM\n",
    "        portfolio_name = f\"Growth_{aum_name.replace(' ', '_')}\"\n",
    "        processed_portfolio = process_portfolio(top_growth_stocks, last_prices, aum_value, 'Growth')\n",
    "        portfolios[portfolio_name] = processed_portfolio\n",
    "        \n",
    "        # Process Value Portfolio for the current AUM\n",
    "        portfolio_name = f\"Value_{aum_name.replace(' ', '_')}\"\n",
    "        processed_portfolio = process_portfolio(top_value_stocks, last_prices, aum_value, 'Value')\n",
    "        portfolios[portfolio_name] = processed_portfolio\n",
    "        \n",
    "        # Process Diversified Portfolio for the current AUM\n",
    "        portfolio_name = f\"Diversified_{aum_name.replace(' ', '_')}\"\n",
    "        processed_portfolio = process_portfolio(diversified_portfolio, last_prices, aum_value, 'Diversification')\n",
    "        portfolios[portfolio_name] = processed_portfolio\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print error if portfolio processing fails for a specific AUM\n",
    "        print(f\"Error processing portfolios for {aum_name}: {e}\")\n",
    "\n",
    "# Display example\n",
    "portfolios['Diversified_Small_Portfolio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing Portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize portfolios into categories\n",
    "Categorized_Portfolios = {\n",
    "    \"Large\": [\n",
    "        (\"Normal_Large_Portfolio\", portfolios['Normal_Large_Portfolio']),\n",
    "        (\"Value_Large_Portfolio\", portfolios['Value_Large_Portfolio']),\n",
    "        (\"Growth_Large_Portfolio\", portfolios['Growth_Large_Portfolio']),\n",
    "        (\"Diversified_Large_Portfolio\", portfolios['Diversified_Large_Portfolio'])\n",
    "    ],\n",
    "    \"Medium\": [\n",
    "        (\"Normal_Medium_Portfolio\", portfolios['Normal_Medium_Portfolio']),\n",
    "        (\"Value_Medium_Portfolio\", portfolios['Value_Medium_Portfolio']),\n",
    "        (\"Growth_Medium_Portfolio\", portfolios['Growth_Medium_Portfolio']),\n",
    "        (\"Diversified_Medium_Portfolio\", portfolios['Diversified_Medium_Portfolio'])\n",
    "    ],\n",
    "    \"Small\": [\n",
    "        (\"Normal_Small_Portfolio\", portfolios['Normal_Small_Portfolio']),\n",
    "        (\"Value_Small_Portfolio\", portfolios['Value_Small_Portfolio']),\n",
    "        (\"Growth_Small_Portfolio\", portfolios['Growth_Small_Portfolio']),\n",
    "        (\"Diversified_Small_Portfolio\", portfolios['Diversified_Small_Portfolio'])\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "We perform risk analysis on the constructed portfolios using Monte Carlo simulations to optimize the portfolio weights and calculate risk metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(returns, cov_matrix, num_simulations=10000):\n",
    "    \"\"\"\n",
    "    Perform a Monte Carlo simulation to find the optimal portfolio weights\n",
    "    that maximize the Sharpe ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: Expected returns for each asset.\n",
    "    - cov_matrix: Covariance matrix of asset returns.\n",
    "    - num_simulations: Number of simulations to run (default is 10,000).\n",
    "\n",
    "    Returns:\n",
    "    - optimal_weights: Array of weights that yield the maximum Sharpe ratio.\n",
    "    - results: Array containing portfolio returns, volatilities, and Sharpe ratios.\n",
    "    \"\"\"\n",
    "    num_assets = len(returns)\n",
    "    results = np.zeros((3, num_simulations))  # Initialize array to store results\n",
    "    weights_record = []  # List to keep track of each simulation's weights\n",
    "    \n",
    "    for i in range(num_simulations):\n",
    "        # Generate random weights for the assets and normalize them\n",
    "        weights = np.random.random(num_assets)\n",
    "        weights /= np.sum(weights)\n",
    "        weights_record.append(weights)\n",
    "        \n",
    "        # Calculate portfolio return and volatility\n",
    "        portfolio_return = np.dot(weights, returns)\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "        # Calculate Sharpe ratio, handling division by zero\n",
    "        sharpe_ratio = portfolio_return / portfolio_volatility if portfolio_volatility != 0 else 0\n",
    "        \n",
    "        # Store the results for this simulation\n",
    "        results[0, i] = portfolio_return\n",
    "        results[1, i] = portfolio_volatility\n",
    "        results[2, i] = sharpe_ratio\n",
    "        \n",
    "    # Identify the index of the highest Sharpe ratio\n",
    "    max_sharpe_idx = np.argmax(results[2])\n",
    "    # Retrieve weights that correspond to the maximum Sharpe ratio\n",
    "    optimal_weights = weights_record[max_sharpe_idx]\n",
    "    \n",
    "    return optimal_weights, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Risk Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_matrix(returns_df):\n",
    "    \"\"\"\n",
    "    Calculate the covariance matrix for a DataFrame of asset returns.\n",
    "\n",
    "    Parameters:\n",
    "    - returns_df: DataFrame of asset returns.\n",
    "\n",
    "    Returns:\n",
    "    - Covariance matrix of the asset returns.\n",
    "    \"\"\"\n",
    "    return returns_df.cov()\n",
    "\n",
    "def calculate_daily_returns_portfolio(portfolio_df):\n",
    "    \"\"\"\n",
    "    Extract daily returns for a given portfolio's tickers.\n",
    "\n",
    "    Parameters:\n",
    "    - portfolio_df: DataFrame with a column 'Ticker' listing portfolio tickers.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame of daily returns for the portfolio's valid tickers.\n",
    "    \"\"\"\n",
    "    tickers = portfolio_df['Ticker'].tolist()\n",
    "    # Filter to include only tickers present in daily_returns\n",
    "    valid_tickers = [ticker for ticker in tickers if ticker in daily_returns.columns]\n",
    "    portfolio_returns = daily_returns[valid_tickers]\n",
    "    return portfolio_returns\n",
    "\n",
    "def calculate_var_es(daily_returns, weights, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk (VaR) and Expected Shortfall (ES) for a portfolio.\n",
    "\n",
    "    Parameters:\n",
    "    - daily_returns: DataFrame of daily returns.\n",
    "    - weights: Array of portfolio weights.\n",
    "    - confidence_level: Confidence level for VaR (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "    - var: Value at Risk at the specified confidence level.\n",
    "    - es: Expected Shortfall at the specified confidence level.\n",
    "    \"\"\"\n",
    "    # Calculate portfolio returns using the weights\n",
    "    portfolio_returns = daily_returns.dot(weights)\n",
    "    # Sort returns to find the VaR threshold\n",
    "    sorted_returns = np.sort(portfolio_returns)\n",
    "    index = int((1 - confidence_level) * len(sorted_returns))\n",
    "    var = sorted_returns[index]\n",
    "    es = sorted_returns[:index].mean()\n",
    "    return var, es\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.005):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe ratio for a series of returns.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: Series of returns.\n",
    "    - risk_free_rate: Annual risk-free rate (default is 0.005).\n",
    "\n",
    "    Returns:\n",
    "    - sharpe_ratio: Sharpe ratio of the returns.\n",
    "    \"\"\"\n",
    "    daily_risk_free_rate = risk_free_rate / 252  # Convert annual to daily rate\n",
    "    excess_returns = returns - daily_risk_free_rate\n",
    "    avg_excess_return = excess_returns.mean()\n",
    "    std_excess_return = excess_returns.std()\n",
    "    sharpe_ratio = avg_excess_return / std_excess_return if std_excess_return != 0 else 0\n",
    "    return sharpe_ratio\n",
    "\n",
    "def calculate_sortino_ratio(returns, risk_free_rate=0.005):\n",
    "    \"\"\"\n",
    "    Calculate the Sortino ratio for a series of returns.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: Series of returns.\n",
    "    - risk_free_rate: Annual risk-free rate (default is 0.005).\n",
    "\n",
    "    Returns:\n",
    "    - sortino_ratio: Sortino ratio of the returns.\n",
    "    \"\"\"\n",
    "    daily_risk_free_rate = risk_free_rate / 252  # Convert annual to daily rate\n",
    "    excess_returns = returns - daily_risk_free_rate\n",
    "    negative_returns = excess_returns[excess_returns < 0]\n",
    "    downside_std = negative_returns.std()\n",
    "    avg_excess_return = excess_returns.mean()\n",
    "    sortino_ratio = avg_excess_return / downside_std if downside_std != 0 else 0\n",
    "    return sortino_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Expected Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected returns for each portfolio and store in a dictionary for monte carlo\n",
    "expected_returns_dict = {}\n",
    "\n",
    "# Loop through each category of portfolios\n",
    "for category, portfolios_list in Categorized_Portfolios.items():\n",
    "    expected_returns_dict[category] = {}\n",
    "    # Loop through each portfolio within the category\n",
    "    for portfolio_name, portfolio_df in portfolios_list:\n",
    "        # Calculate daily returns for the portfolio and store in the dictionary\n",
    "        expected_returns_dict[category][portfolio_name] = calculate_daily_returns_portfolio(portfolio_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Monte Carlo Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Monte Carlo simulations for each portfolio and store results\n",
    "monte_carlo_results = {}\n",
    "\n",
    "# Loop through each portfolio category\n",
    "for category, portfolios_list in Categorized_Portfolios.items():\n",
    "    monte_carlo_results[category] = {}\n",
    "    # Loop through each portfolio within the category\n",
    "    for portfolio_name, portfolio_df in portfolios_list:\n",
    "        # Get daily returns for the current portfolio\n",
    "        daily_returns_portfolio = expected_returns_dict[category][portfolio_name]\n",
    "        if daily_returns_portfolio.empty:\n",
    "            continue  # Skip simulation if no valid returns available\n",
    "        \n",
    "        # Calculate mean returns and covariance matrix for Monte Carlo inputs\n",
    "        returns_mean = daily_returns_portfolio.mean()\n",
    "        cov_matrix = get_covariance_matrix(daily_returns_portfolio)\n",
    "        \n",
    "        # Run Monte Carlo simulation to get optimal weights and results\n",
    "        optimal_weights, results = monte_carlo_simulation(returns_mean, cov_matrix)\n",
    "        \n",
    "        # Store the simulation outcomes in the results dictionary\n",
    "        monte_carlo_results[category][portfolio_name] = {\n",
    "            'optimal_weights': optimal_weights,\n",
    "            'simulation_results': results\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing New Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the returns of each optimized portfolio\n",
    "portfolio_returns_list = []\n",
    "portfolio_names = []\n",
    "\n",
    "for category in Categorized_Portfolios:\n",
    "    for portfolio_name, portfolio_df in Categorized_Portfolios[category]:\n",
    "        # Check if the portfolio has optimization results\n",
    "        if portfolio_name not in monte_carlo_results[category]:\n",
    "            continue  # Skip if no simulation results\n",
    "        \n",
    "        # Get the optimized weights\n",
    "        optimal_weights = monte_carlo_results[category][portfolio_name]['optimal_weights']\n",
    "        \n",
    "        # Get the daily returns for the portfolio\n",
    "        daily_returns_portfolio = expected_returns_dict[category][portfolio_name]\n",
    "        \n",
    "        # Check if optimal_weights and daily_returns_portfolio are valid\n",
    "        if optimal_weights is None or daily_returns_portfolio.empty:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the portfolio's daily returns using the optimized weights\n",
    "        weighted_daily_returns = daily_returns_portfolio.dot(optimal_weights)\n",
    "        \n",
    "        # Store the portfolio returns\n",
    "        portfolio_returns_list.append(weighted_daily_returns)\n",
    "        portfolio_names.append(portfolio_name)\n",
    "\n",
    "# Create a DataFrame of portfolio returns\n",
    "portfolio_returns_df = pd.concat(portfolio_returns_list, axis=1)\n",
    "portfolio_returns_df.columns = portfolio_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Risk Metrics Calculation\n",
    "# Calculate risk metrics for each portfolio\n",
    "portfolio_risk_metrics = {}\n",
    "\n",
    "# Loop through each portfolio category\n",
    "for category, portfolios_list in Categorized_Portfolios.items():\n",
    "    portfolio_risk_metrics[category] = {}\n",
    "    # Loop through each portfolio within the category\n",
    "    for portfolio_name, portfolio_df in portfolios_list:\n",
    "        if portfolio_name not in monte_carlo_results[category]:\n",
    "            continue  # Skip if no simulation results available\n",
    "        \n",
    "        # Retrieve daily returns and optimal weights for the portfolio\n",
    "        daily_returns_portfolio = expected_returns_dict[category][portfolio_name]\n",
    "        optimal_weights = monte_carlo_results[category][portfolio_name]['optimal_weights']\n",
    "        \n",
    "        # Calculate weighted daily returns based on optimal weights\n",
    "        weighted_daily_returns = daily_returns_portfolio.dot(optimal_weights)\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        var, es = calculate_var_es(daily_returns_portfolio, optimal_weights)\n",
    "        volatility = weighted_daily_returns.std()\n",
    "        sharpe_ratio = calculate_sharpe_ratio(weighted_daily_returns, risk_free_rate)\n",
    "        sortino_ratio = calculate_sortino_ratio(weighted_daily_returns, risk_free_rate)\n",
    "        \n",
    "        # Store calculated metrics in the results dictionary\n",
    "        portfolio_risk_metrics[category][portfolio_name] = {\n",
    "            'VaR': var,\n",
    "            'ES': es,\n",
    "            'Volatility': volatility,\n",
    "            'Sharpe_Ratio': sharpe_ratio,\n",
    "            'Sortino_Ratio': sortino_ratio\n",
    "        }\n",
    "\n",
    "# Collect portfolio volatilities and names for further analysis\n",
    "portfolio_volatilities = []\n",
    "portfolio_names = []\n",
    "\n",
    "for category in portfolio_risk_metrics:\n",
    "    for portfolio_name in portfolio_risk_metrics[category]:\n",
    "        volatility = portfolio_risk_metrics[category][portfolio_name]['Volatility']\n",
    "        portfolio_volatilities.append(volatility)\n",
    "        portfolio_names.append(portfolio_name)\n",
    "\n",
    "# Convert portfolio volatilities to a NumPy array for further calculations\n",
    "portfolio_volatilities = np.array(portfolio_volatilities)\n",
    "\n",
    "# Print out the calculated risk metrics for each portfolio\n",
    "for category in portfolio_risk_metrics:\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for portfolio_name in portfolio_risk_metrics[category]:\n",
    "        metrics = portfolio_risk_metrics[category][portfolio_name]\n",
    "        print(f\"Portfolio: {portfolio_name}\")\n",
    "        print(f\"  Sharpe Ratio: {metrics['Sharpe_Ratio']:.2f}\")\n",
    "        print(f\"  Sortino Ratio: {metrics['Sortino_Ratio']:.2f}\")\n",
    "        print(f\"  Volatility: {metrics['Volatility']:.2%}\")\n",
    "        print(f\"  VaR: {metrics['VaR']:.2%}\")\n",
    "        print(f\"  ES: {metrics['ES']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sharpe and Sortino Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for plotting\n",
    "portfolios = []\n",
    "sharpe_ratios = []\n",
    "sortino_ratios = []\n",
    "\n",
    "for category in portfolio_risk_metrics:\n",
    "    for portfolio_name in portfolio_risk_metrics[category]:\n",
    "        metrics = portfolio_risk_metrics[category][portfolio_name]\n",
    "        portfolios.append(f\"{category} - {portfolio_name}\")\n",
    "        sharpe_ratios.append(metrics['Sharpe_Ratio'])\n",
    "        sortino_ratios.append(metrics['Sortino_Ratio'])\n",
    "\n",
    "# Plot Sharpe Ratios\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(portfolios, sharpe_ratios, color='blue', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.title('Sharpe Ratios of Portfolios')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Sortino Ratios\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(portfolios, sortino_ratios, color='green', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Sortino Ratio')\n",
    "plt.title('Sortino Ratios of Portfolios')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fund of Funds (FoF) Construction\n",
    "\n",
    "We construct a Fund of Funds by combining individual portfolios based on their volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating FoF Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_volatility_weights(volatilities):\n",
    "    \"\"\"\n",
    "    Calculate inverse volatility weights without any maximum weight constraint.\n",
    "\n",
    "    Parameters:\n",
    "    - volatilities: numpy array of portfolio volatilities.\n",
    "\n",
    "    Returns:\n",
    "    - weights: numpy array of weights that are proportional to the inverse of volatilities.\n",
    "    \"\"\"\n",
    "    # Calculate the inverse of each volatility value\n",
    "    inv_vols = 1 / volatilities\n",
    "    # Normalize to ensure weights sum to 1\n",
    "    weights = inv_vols / np.sum(inv_vols)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def inverse_volatility_weights_with_constraint(volatilities, max_weight=0.10):\n",
    "    \"\"\"\n",
    "    Calculate inverse volatility weights with a maximum weight constraint.\n",
    "\n",
    "    Parameters:\n",
    "    - volatilities: numpy array of portfolio volatilities.\n",
    "    - max_weight: maximum allowable weight for any portfolio.\n",
    "\n",
    "    Returns:\n",
    "    - weights: numpy array of adjusted weights.\n",
    "    \"\"\"\n",
    "    inv_vols = 1 / volatilities\n",
    "    weights = inv_vols / np.sum(inv_vols)\n",
    "\n",
    "    # Initialize variables for the iterative process\n",
    "    weights = np.minimum(weights, max_weight)\n",
    "    total_weight = np.sum(weights)\n",
    "    iteration = 0\n",
    "    max_iterations = 100  # Safety cap to prevent infinite loops\n",
    "\n",
    "    # Iteratively redistribute excess weight\n",
    "    while total_weight < 1.0 and iteration < max_iterations:\n",
    "        # Calculate the weights below the maximum limit\n",
    "        below_max = weights < max_weight\n",
    "        # Calculate the total weight below the maximum limit\n",
    "        total_below_max = np.sum(weights[below_max])\n",
    "        # If no weights are below max, break to avoid division by zero\n",
    "        if total_below_max == 0:\n",
    "            break\n",
    "        # Calculate the amount of weight to redistribute\n",
    "        remaining_weight = 1.0 - total_weight\n",
    "        # Redistribute the remaining weight proportionally\n",
    "        redistribution = remaining_weight * (weights[below_max] / total_below_max)\n",
    "        weights[below_max] += redistribution\n",
    "        # Reapply the maximum weight constraint\n",
    "        weights = np.minimum(weights, max_weight)\n",
    "        total_weight = np.sum(weights)\n",
    "        iteration += 1\n",
    "\n",
    "    # Normalize the weights to sum to 1 (in case of rounding errors)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating FoF Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fof_performance(weights, portfolio_returns_df):\n",
    "    \"\"\"\n",
    "    Simulate the Fund of Funds (FoF) performance by computing the weighted sum of portfolio returns.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: NumPy array of portfolio weights.\n",
    "    - portfolio_returns_df: DataFrame of portfolio daily returns, where each column is a portfolio.\n",
    "\n",
    "    Returns:\n",
    "    - fof_returns: Series of FoF daily returns.\n",
    "    \"\"\"\n",
    "    # Ensure that the weights and portfolio returns align\n",
    "    if len(weights) != portfolio_returns_df.shape[1]:\n",
    "        raise ValueError(\"Number of weights must match the number of portfolios.\")\n",
    "    \n",
    "    # Calculate the FoF returns\n",
    "    fof_returns = portfolio_returns_df.dot(weights)\n",
    "    return fof_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating FoF Risk Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var_es_fof(returns, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk (VaR) and Expected Shortfall (ES) for a series of returns.\n",
    "\n",
    "    Parameters:\n",
    "    - returns: Series of portfolio returns.\n",
    "    - confidence_level: Confidence level for VaR (default is 0.95).\n",
    "\n",
    "    Returns:\n",
    "    - var: Value at Risk at the specified confidence level.\n",
    "    - es: Expected Shortfall at the specified confidence level.\n",
    "    \"\"\"\n",
    "    if returns.empty:\n",
    "        return np.nan, np.nan  # Return NaN if no returns are available\n",
    "    \n",
    "    # Sort returns to find the VaR threshold\n",
    "    sorted_returns = np.sort(returns)\n",
    "    index = int((1 - confidence_level) * len(sorted_returns))\n",
    "    var = sorted_returns[index]\n",
    "    es = sorted_returns[:index].mean()  # Average of returns below the VaR threshold\n",
    "    return var, es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing FoF Construction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that portfolio_returns_df columns match the order of portfolio_names\n",
    "portfolio_returns_df = portfolio_returns_df[portfolio_names]\n",
    "\n",
    "# Calculate portfolio volatilities (standard deviations of returns)\n",
    "portfolio_volatilities = np.array([portfolio_returns_df[portfolio].std() for portfolio in portfolio_names])\n",
    "\n",
    "# Calculate FoF weights without constraints\n",
    "fof_weights = inverse_volatility_weights(portfolio_volatilities)\n",
    "\n",
    "# If you have a maximum weight constraint, you can apply it\n",
    "max_weight = 0.10  # 10% maximum weight per portfolio\n",
    "adjusted_weights = inverse_volatility_weights_with_constraint(portfolio_volatilities, max_weight)\n",
    "\n",
    "# Create DataFrames of the weights\n",
    "adjusted_weights_df = pd.DataFrame({\n",
    "    'Portfolio': portfolio_names,\n",
    "    'Adjusted Weight': adjusted_weights\n",
    "})\n",
    "\n",
    "fofWeights_df = pd.DataFrame({\n",
    "    'Portfolio': portfolio_names,\n",
    "    'Non-Adjusted Weight': fof_weights\n",
    "})\n",
    "\n",
    "\n",
    "# Plot initial inverse volatility weights vs. adjusted weights\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(portfolio_names, fof_weights, alpha=0.5, label='Initial Weights')\n",
    "plt.bar(portfolio_names, adjusted_weights, alpha=0.5, label='Adjusted Weights with Constraints')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Comparison of Initial and Adjusted Inverse Volatility Weights')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate FoF returns using the updated simulate_fof_performance function\n",
    "fof_returns = simulate_fof_performance(fof_weights, portfolio_returns_df)\n",
    "\n",
    "# Calculate FoF risk metrics\n",
    "fof_var, fof_es = calculate_var_es_fof(fof_returns)\n",
    "fof_volatility = fof_returns.std()\n",
    "fof_sharpe_ratio = calculate_sharpe_ratio(fof_returns, risk_free_rate)\n",
    "fof_sortino_ratio = calculate_sortino_ratio(fof_returns, risk_free_rate)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFoF with Inverse Volatility Weights:\")\n",
    "print(f\"VaR: {fof_var:.2%}, ES: {fof_es:.2%}\")\n",
    "print(f\"Volatility: {fof_volatility:.2%}\")\n",
    "print(f\"Sharpe Ratio: {fof_sharpe_ratio:.2f}\")\n",
    "print(f\"Sortino Ratio: {fof_sortino_ratio:.2f}\")\n",
    "\n",
    "# Calculate the covariance matrix of portfolio returns\n",
    "fof_cov_matrix = portfolio_returns_df.cov()\n",
    "\n",
    "# The weights are already in fof_weights (a NumPy array)\n",
    "inverse_vol_weights = fof_weights\n",
    "\n",
    "# Calculate the FoF's total volatility\n",
    "fof_volatility = np.sqrt(np.dot(inverse_vol_weights.T, np.dot(fof_cov_matrix.values, inverse_vol_weights)))\n",
    "\n",
    "# Calculate marginal risk contribution of each portfolio\n",
    "marginal_contributions = np.dot(fof_cov_matrix.values, inverse_vol_weights)\n",
    "\n",
    "# Calculate total risk contribution of each portfolio\n",
    "risk_contributions = inverse_vol_weights * marginal_contributions / fof_volatility\n",
    "\n",
    "# Calculate percentage risk contributions\n",
    "percentage_risk_contributions = risk_contributions / fof_volatility\n",
    "\n",
    "# Optionally, print risk contributions\n",
    "risk_contributions_df = pd.DataFrame({\n",
    "    'Portfolio': portfolio_names,\n",
    "    'Weight': inverse_vol_weights,\n",
    "    'Risk Contribution': risk_contributions,\n",
    "    'Percentage Risk Contribution': percentage_risk_contributions\n",
    "})\n",
    "\n",
    "# Calculate the FoF returns using the adjusted weights\n",
    "fof_returns_adjusted = simulate_fof_performance(adjusted_weights, portfolio_returns_df)\n",
    "\n",
    "# Calculate risk metrics for the adjusted FoF\n",
    "fof_var_adj, fof_es_adj = calculate_var_es_fof(fof_returns_adjusted)\n",
    "fof_volatility_adj = fof_returns_adjusted.std()\n",
    "fof_sharpe_ratio_adj = calculate_sharpe_ratio(fof_returns_adjusted, risk_free_rate)\n",
    "fof_sortino_ratio_adj = calculate_sortino_ratio(fof_returns_adjusted, risk_free_rate)\n",
    "\n",
    "# Print the adjusted results\n",
    "print(\"\\nFoF with Adjusted Inverse Volatility Weights:\")\n",
    "print(f\"VaR: {fof_var_adj:.2%}, ES: {fof_es_adj:.2%}\")\n",
    "print(f\"Volatility: {fof_volatility_adj:.2%}\")\n",
    "print(f\"Sharpe Ratio: {fof_sharpe_ratio_adj:.2f}\")\n",
    "print(f\"Sortino Ratio: {fof_sortino_ratio_adj:.2f}\")\n",
    "\n",
    "# Calculate the covariance matrix of portfolio returns\n",
    "fof_cov_matrix = portfolio_returns_df.cov()\n",
    "\n",
    "# Use the adjusted weights (a NumPy array)\n",
    "adjusted_inv_vol_weights = adjusted_weights\n",
    "\n",
    "# Calculate the FoF's total volatility with adjusted weights\n",
    "fof_volatility_adj = np.sqrt(np.dot(adjusted_inv_vol_weights.T, np.dot(fof_cov_matrix.values, adjusted_inv_vol_weights)))\n",
    "\n",
    "# Calculate marginal risk contribution of each portfolio\n",
    "marginal_contributions_adj = np.dot(fof_cov_matrix.values, adjusted_inv_vol_weights)\n",
    "\n",
    "# Calculate total risk contribution of each portfolio\n",
    "risk_contributions_adj = adjusted_inv_vol_weights * marginal_contributions_adj / fof_volatility_adj\n",
    "\n",
    "# Calculate percentage risk contributions\n",
    "percentage_risk_contributions_adj = risk_contributions_adj / fof_volatility_adj\n",
    "\n",
    "# Optionally, print risk contributions\n",
    "risk_contributions_adj_df = pd.DataFrame({\n",
    "    'Portfolio': portfolio_names,\n",
    "    'Weight': adjusted_inv_vol_weights,\n",
    "    'Risk Contribution': risk_contributions_adj,\n",
    "    'Percentage Risk Contribution': percentage_risk_contributions_adj\n",
    "})\n",
    "\n",
    "print(\"\\nRisk Contributions for adjusted:\")\n",
    "risk_contributions_adj_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we successfully constructed and analyzed various investment portfolios using different strategies and AUM sizes. By performing risk analysis and optimization:\n",
    "\n",
    "- **Portfolio Diversification**: We demonstrated the benefits of diversification through different strategies and the construction of a Fund of Funds.\n",
    "- **Risk Management**: Calculated key risk metrics (VaR, ES, Volatility) and risk-adjusted performance metrics (Sharpe Ratio, Sortino Ratio) to understand the risk profile and performance of each portfolio.\n",
    "- **Risk-Adjusted Performance**: The Sharpe and Sortino Ratios provided insights into how well each portfolio compensated investors for the risk taken, especially focusing on downside risk with the Sortino Ratio.\n",
    "- **Optimization with Constraints**: Implemented a maximum weight constraint in the FoF construction to prevent over-concentration in any single portfolio, enhancing diversification and adhering to risk management practices.\n",
    "- **Optimization**: Used Monte Carlo simulations and adjusted inverse volatility weighting to optimize portfolio weights, enhancing the risk-adjusted returns.\n",
    "\n",
    "My analysis revealed that incorporating a maximum weight constraint in the FoF can lead to a more balanced portfolio allocation without significantly compromising performance. This highlights the importance of considering practical constraints in portfolio construction to manage risk effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The execution of this notebook may take some time due to data fetching and computations. Ensure you have a stable internet connection and the required packages installed.\n",
    "\n",
    "**Packages Required**:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- yfinance\n",
    "- matplotlib (optional for visualizations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
